---
layout: post
title: The unreasonable effectiveness of cooking
comments: true
---

Recently, I found myself speaking to a room full of folks who were mostly from the business world about Deep Learning. In preparation for this, I had to convince myself to sound technically wrong at times in the interest of  getting across a particular concept - say, a softmax or RNNs or matrix multiplications and most interestingly back propagation.
 
After many attempts to draw references to commonplace practices and processes on the fly, I stumbled upon a rather amusing method of explaining rudimentary machine learning which all of us quite familiar with - cooking. That's right, if you are someone who understands machine learning, you'd quickly begin to visualize and appreciate that, both deep learning and cooking have a lot in common. It'll almost be like a lego block puzzle solving itself.
 
Let's consider an example just in case you're not yet sold on the idea. All of us can agree any ML/DL method would broadly have these four steps: 

- 1. Pre-processing data 
- 2. Training a model 
- 3. Deploying the model and 
- 4. Collecting the error feedback. 

Now if we have to compare this with cooking; Pre-processing data = washing, slicing, dicing vegetables, Training a model = putting the vegetables in a pan while boiling/cooking, Deploying the model = getting someone to eat the dish and finally Collecting error feedback = capturing the person's reactions to improve upon later. Adding salt to taste can be considered as hyperparameter tuning resulting in building better models.

![DataToValue](/public/images/postImages/cooking/dataToValue_2.png "data2Value image")
 
Now that I've empirically proved to myself that cooking is a helpful way in remembering about Deep Learning, I'm beginning to find lot of clarity in drawing references. The whole essence of 'going deep' can be neatly indexed to preparing rice in a pressure cooker. Just like you can't see the rice getting cooked and you don't hand-craft features anymore. We learn representations. Pressure cooker = Deep Learning (sort of a black box) seems legit to me!
 
I've decided to adopt this strategy of cross-referencing cooking with Deep Learning while attempting to make it easily understandable for non-Stats/CompSci/ML/DL folks. This post shall remind me of the genesis of this experiment in case I need to run a stack trace to correct my methods.